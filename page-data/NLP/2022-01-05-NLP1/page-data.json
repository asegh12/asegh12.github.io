{
    "componentChunkName": "component---src-templates-post-template-tsx",
    "path": "/NLP/2022-01-05-NLP1/",
    "result": {"data":{"allMarkdownRemark":{"edges":[{"node":{"html":"<h2>Introduction</h2>\n<p>머신러닝에 대한 이야기 → 대부분은 지도학습의 패러다임으로 바꾼다. → 지도학습을 제대로 알아보자.</p>\n<h2>Basic ML : OverView</h2>\n<h3>머신러닝</h3>\n<ul>\n<li>알고리즘을 푸는데에 머신러닝은 새로운 패러다임이 될 수 있다.</li>\n<li>알고리즘 : Instruction set(일련의 명령들)을 어떻게 구성해서 문제를 풀어주는 답을 찾을것인가</li>\n<li>머신러닝 : 문제의 Specification이 명확하지 않는경우가 많음.(얼굴로 예를 들면 앞, 옆, 뒤 등등 많은 상황) → 예시가 있다면 머신러닝의 함수(알고리즘)을 찾을 수 있다. → 알고리즘과 반대로 예시와 답으로 알고리즘을 찾는다.</li>\n</ul>\n<h3>지도학습(Supervised Learning) :</h3>\n<ul>\n<li>Provided :\n<ol>\n<li>a set N input-output “training” examples</li>\n<li>A per-example loss function</li>\n<li>Evaluation sets(validation, test)</li>\n</ol>\n</li>\n<li>What we must decide :\n<ol>\n<li>Hypothesis sets(가설 집합) → 회귀(SVR, 선형 회귀 등)와 분류(SVM, 로지스틱 회귀 등)</li>\n<li>Optimization algorithm</li>\n</ol>\n</li>\n<li><code class=\"language-text\">Train, Val, Test set</code> / <code class=\"language-text\">Loss function</code> / <code class=\"language-text\">Hypothesis sets</code> / <code class=\"language-text\">Optimization algorithm</code></li>\n<li>Test set 나누는 것이 굉장히 중요하다.</li>\n<li>3가지 포인트 : 어떤 가설집합을 써야할지, 어떤 손실함수를 써야할지, 어떤 최적화기법을 써야할지</li>\n</ul>\n<h2>Basic ML : Hypothesis Set</h2>\n<h3>가설집합은 무한히 많다.</h3>\n<ul>\n<li>Classification(분류)는 SVM, Naive bayes classifier, Logistic Regression 등..</li>\n<li>Regression(회귀)는 SVR, Linear Regression, Gaussian Process 등..</li>\n<li>또한 가설집합을 선택하여도, 가설집합의 하이퍼 파라미터에 따라서 최적화될 파라미터의 개수는 변화된다.</li>\n</ul>\n<h3>Nerual Network에서의 Hypothesis Set의 선택엔 두 가지를 결정해야 함.</h3>\n<ul>\n<li><code class=\"language-text\">Network Architeucture의 선택</code>, <code class=\"language-text\">Architeucture 내의 파라미터 값</code></li>\n</ul>\n<p>→ 하나의 가설집합에서도 무한한 모델이 있다는 의미</p>\n<h3>Nerual Network는 DAG(방향이 있는 비순환 그래프)이다.</h3>\n<ul>\n<li>Input &#x26; output vector, Parameter variable, Computing node</li>\n<li>위의 노드 요소들을 DAG 형태로 그려볼 수 있다.</li>\n</ul>\n<img src = \"/assets/images/DAG.png\">  \n<h3>Classification 예제 - 로지스틱 회귀의 DAG</h3>\n<ul>\n<li>로지스틱 회귀(Logistic Regression) : 어떤 input에 대한 이진분류</li>\n</ul>\n<img src = \"/assets/images/lgm.png\">  \n<br>\n<img src = \"/assets/images/lg.png\">\n<ul>\n<li>그림 설계\n<ol>\n<li>input은 x이며 점선 원</li>\n<li>x가 향하는 노드는 내적을 수행하는 노드이며 사각형</li>\n<li>w, b는 파라미터 변수로 실선 원</li>\n<li>sum을 수행하는 computing 노드에서는 앞 노드의 결과값과 두번째 파라미터 b를 더해줌.</li>\n<li>결과를 시그모이드 함수로 보냄</li>\n</ol>\n</li>\n<li>Forward Computation : x, w, b의 노드가 존재 → 곱하기 계산 → 더하기 계산 → 시그모이드 함수 노드 적용</li>\n</ul>\n<h3>DAG로 (Abstraction)한 것의 장점</h3>\n<ul>\n<li>Neraul Network Architrecture에 대한 전체적인 이해가 쉽다.</li>\n<li>실제 Back-end에서 CPU or GPU를 쓰는지 해당 아키텍쳐의 이해를 위해 굳이 알 필요가 없다.</li>\n<li>모든 노드들의 코드를 그대로 사용할 수 있어 reusability가 높다.</li>\n</ul>\n<blockquote>\n<p><a href=\"https://wikidocs.net\" target=\"_blank\" rel=\"nofollow\">위키독스</a>의 <code class=\"language-text\">딥 러닝을 위한 자연어 처리 심화</code> 문서를 참조했고, <a href=\"https://www.boostcourse.org\" target=\"_blank\" rel=\"nofollow\">boostcourse</a>의 <code class=\"language-text\">딥러닝을 이용한 자연어 처리</code> 강의를 보았습니다.</p>\n</blockquote>","frontmatter":{"title":"딥러닝을 이용한 자연어 처리 강의 요약 정리 (1)","summary":"Introduce, Basic ML - Overview, Hypothesis Set","date":null,"categories":["딥러닝을 이용한 자연어 처리"],"thumbnail":null},"tableOfContents":"<ul>\n<li>\n<p><a href=\"#introduction\">Introduction</a></p>\n</li>\n<li>\n<p><a href=\"#basic-ml--overview\">Basic ML : OverView</a></p>\n<ul>\n<li><a href=\"#%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D\">머신러닝</a></li>\n<li><a href=\"#%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5supervised-learning-\">지도학습(Supervised Learning) :</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#basic-ml--hypothesis-set\">Basic ML : Hypothesis Set</a></p>\n<ul>\n<li><a href=\"#%EA%B0%80%EC%84%A4%EC%A7%91%ED%95%A9%EC%9D%80-%EB%AC%B4%ED%95%9C%ED%9E%88-%EB%A7%8E%EB%8B%A4\">가설집합은 무한히 많다.</a></li>\n<li><a href=\"#nerual-network%EC%97%90%EC%84%9C%EC%9D%98-hypothesis-set%EC%9D%98-%EC%84%A0%ED%83%9D%EC%97%94-%EB%91%90-%EA%B0%80%EC%A7%80%EB%A5%BC-%EA%B2%B0%EC%A0%95%ED%95%B4%EC%95%BC-%ED%95%A8\">Nerual Network에서의 Hypothesis Set의 선택엔 두 가지를 결정해야 함.</a></li>\n<li><a href=\"#nerual-network%EB%8A%94-dag%EB%B0%A9%ED%96%A5%EC%9D%B4-%EC%9E%88%EB%8A%94-%EB%B9%84%EC%88%9C%ED%99%98-%EA%B7%B8%EB%9E%98%ED%94%84%EC%9D%B4%EB%8B%A4\">Nerual Network는 DAG(방향이 있는 비순환 그래프)이다.</a></li>\n<li><a href=\"#classification-%EC%98%88%EC%A0%9C---%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1-%ED%9A%8C%EA%B7%80%EC%9D%98-dag\">Classification 예제 - 로지스틱 회귀의 DAG</a></li>\n<li><a href=\"#dag%EB%A1%9C-abstraction%ED%95%9C-%EA%B2%83%EC%9D%98-%EC%9E%A5%EC%A0%90\">DAG로 (Abstraction)한 것의 장점</a></li>\n</ul>\n</li>\n</ul>"}}]}},"pageContext":{"slug":"/NLP/2022-01-05-NLP1/"}},
    "staticQueryHashes": ["3649515864"]}